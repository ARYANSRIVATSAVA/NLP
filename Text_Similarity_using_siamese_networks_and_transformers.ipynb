{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPcvz4DdzcAiA3rmvVlVTZP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ARYANSRIVATSAVA/NLP/blob/main/Text_Similarity_using_siamese_networks_and_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlGjGoGMTt-D"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# Import Trax\n",
        "\n",
        "!pip install -q -U trax\n",
        "import trax"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nltk\n",
        "from trax import layers as tl\n",
        "from trax.supervised import training\n",
        "from trax.fastmath import numpy as fastnp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random as rnd\n",
        "\n",
        "# set random seeds\n",
        "#trax.supervised.trainer_lib.init_random_number_generators(34)\n",
        "rnd.seed(34)"
      ],
      "metadata": {
        "id": "35WG5LzHeIJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import files\n",
        "#uploaded = files.upload()"
      ],
      "metadata": {
        "id": "CmZ7Cls4f1oo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"/content/train.csv\")\n",
        "N=len(data)\n",
        "print('Number of question pairs: ', N)\n",
        "data.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "id": "Maeqay8hogUe",
        "outputId": "09ff4bbd-1f7a-4163-a51b-9371861f13a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of question pairs:  404290\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id  qid1  qid2                                          question1  \\\n",
              "0   0     1     2  What is the step by step guide to invest in sh...   \n",
              "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
              "2   2     5     6  How can I increase the speed of my internet co...   \n",
              "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
              "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
              "5   5    11    12  Astrology: I am a Capricorn Sun Cap moon and c...   \n",
              "6   6    13    14                                Should I buy tiago?   \n",
              "7   7    15    16                     How can I be a good geologist?   \n",
              "8   8    17    18                    When do you use シ instead of し?   \n",
              "9   9    19    20  Motorola (company): Can I hack my Charter Moto...   \n",
              "\n",
              "                                           question2  is_duplicate  \n",
              "0  What is the step by step guide to invest in sh...             0  \n",
              "1  What would happen if the Indian government sto...             0  \n",
              "2  How can Internet speed be increased by hacking...             0  \n",
              "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
              "4            Which fish would survive in salt water?             0  \n",
              "5  I'm a triple Capricorn (Sun, Moon and ascendan...             1  \n",
              "6  What keeps childern active and far from phone ...             0  \n",
              "7          What should I do to be a great geologist?             1  \n",
              "8              When do you use \"&\" instead of \"and\"?             0  \n",
              "9  How do I hack Motorola DCX3400 for free internet?             0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-351a86eb-ff7c-4152-9f00-15d3b3dda41d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>qid1</th>\n",
              "      <th>qid2</th>\n",
              "      <th>question1</th>\n",
              "      <th>question2</th>\n",
              "      <th>is_duplicate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>What is the step by step guide to invest in sh...</td>\n",
              "      <td>What is the step by step guide to invest in sh...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
              "      <td>What would happen if the Indian government sto...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>How can I increase the speed of my internet co...</td>\n",
              "      <td>How can Internet speed be increased by hacking...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
              "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>10</td>\n",
              "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
              "      <td>Which fish would survive in salt water?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>11</td>\n",
              "      <td>12</td>\n",
              "      <td>Astrology: I am a Capricorn Sun Cap moon and c...</td>\n",
              "      <td>I'm a triple Capricorn (Sun, Moon and ascendan...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>13</td>\n",
              "      <td>14</td>\n",
              "      <td>Should I buy tiago?</td>\n",
              "      <td>What keeps childern active and far from phone ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>15</td>\n",
              "      <td>16</td>\n",
              "      <td>How can I be a good geologist?</td>\n",
              "      <td>What should I do to be a great geologist?</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>17</td>\n",
              "      <td>18</td>\n",
              "      <td>When do you use シ instead of し?</td>\n",
              "      <td>When do you use \"&amp;\" instead of \"and\"?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>19</td>\n",
              "      <td>20</td>\n",
              "      <td>Motorola (company): Can I hack my Charter Moto...</td>\n",
              "      <td>How do I hack Motorola DCX3400 for free internet?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-351a86eb-ff7c-4152-9f00-15d3b3dda41d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-351a86eb-ff7c-4152-9f00-15d3b3dda41d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-351a86eb-ff7c-4152-9f00-15d3b3dda41d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "N_train = 300000\n",
        "N_test  = 10*1024\n",
        "data_train = data[:N_train]\n",
        "data_test  = data[N_train:N_train+N_test]\n",
        "print(\"Train set:\", len(data_train), \"Test set:\", len(data_test))\n",
        "del(data) # remove to free memory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vl-qvyt5oqxS",
        "outputId": "83e46923-7fa3-4cdb-a747-6287113f2e78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set: 300000 Test set: 10240\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "td_index = (data_train['is_duplicate'] == 1).to_numpy()\n",
        "td_index = [i for i, x in enumerate(td_index) if x]\n",
        "print('number of duplicate questions: ', len(td_index))\n",
        "print('indexes of first ten duplicate questions:', td_index[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6TNyE3QpF8g",
        "outputId": "da5ef41e-f003-45c8-8390-322eda6ec9a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of duplicate questions:  111473\n",
            "indexes of first ten duplicate questions: [5, 7, 11, 12, 13, 15, 16, 18, 20, 29]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_train['question1'][5])\n",
        "print(data_train['question2'][5])\n",
        "print('is_duplicate: ', data_train['is_duplicate'][5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7a4mY-spSgy",
        "outputId": "e4292803-b80d-4da7-a522-e92ac123015f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?\n",
            "I'm a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me?\n",
            "is_duplicate:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Q1_train_words = np.array(data_train['question1'][td_index])\n",
        "Q2_train_words = np.array(data_train['question2'][td_index])\n",
        "\n",
        "Q1_test_words = np.array(data_test['question1'])\n",
        "Q2_test_words = np.array(data_test['question2'])\n",
        "y_test  = np.array(data_test['is_duplicate'])"
      ],
      "metadata": {
        "id": "RDuEzss_pOgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Jj_3hgptflP",
        "outputId": "22b41f6c-7505-4ee6-a5f8-aebacbd51a1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> punkt\n",
            "Command 'punkt' unrecognized\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> punkt\n",
            "    Downloading package punkt to /root/nltk_data...\n",
            "      Unzipping tokenizers/punkt.zip.\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create arrays\n",
        "Q1_train = np.empty_like(Q1_train_words)\n",
        "Q2_train = np.empty_like(Q2_train_words)\n",
        "\n",
        "Q1_test = np.empty_like(Q1_test_words)\n",
        "Q2_test = np.empty_like(Q2_test_words)"
      ],
      "metadata": {
        "id": "FxTrM9dUu9SB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# Building the vocabulary with the train set         (this might take a minute)\n",
        "from collections import defaultdict\n",
        "#punkt module has to be downloaded using nltk.download()\n",
        "vocab = defaultdict(lambda: 0)\n",
        "vocab['<PAD>'] = 1\n",
        "\n",
        "for idx in range(len(Q1_train_words)):\n",
        "    Q1_train[idx] = nltk.word_tokenize(Q1_train_words[idx])\n",
        "    Q2_train[idx] = nltk.word_tokenize(Q2_train_words[idx])\n",
        "    q = Q1_train[idx] + Q2_train[idx]\n",
        "    for word in q:\n",
        "        if word not in vocab:\n",
        "            vocab[word] = len(vocab) + 1\n",
        "print('The length of the vocabulary is: ', len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61o8tANTuY3u",
        "outputId": "7d28e2bf-68f3-4d72-97ba-7cad4077a93c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The length of the vocabulary is:  36352\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Q1_train[5])\n",
        "print(Q2_train[5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GziERw9mvifk",
        "outputId": "d8f44522-fbda-446c-e511-01df582c32c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['What', 'would', 'a', 'Trump', 'presidency', 'mean', 'for', 'current', 'international', 'master', '’', 's', 'students', 'on', 'an', 'F1', 'visa', '?']\n",
            "['How', 'will', 'a', 'Trump', 'presidency', 'affect', 'the', 'students', 'presently', 'in', 'US', 'or', 'planning', 'to', 'study', 'in', 'US', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab['<PAD>'])\n",
        "print(vocab['Astrology'])\n",
        "print(vocab['Astronomy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVf7cLfLu383",
        "outputId": "35cf93b6-5efb-4867-9668-d535d49f31c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for idx in range(len(Q1_test_words)):\n",
        "    Q1_test[idx] = nltk.word_tokenize(Q1_test_words[idx])\n",
        "    Q2_test[idx] = nltk.word_tokenize(Q2_test_words[idx])"
      ],
      "metadata": {
        "id": "XiIKeWVFvNGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Train set has reduced to: ', len(Q1_train) )\n",
        "print('Test set length: ', len(Q1_test) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNsXbHrQvfkG",
        "outputId": "5b432f66-429e-48c4-8b41-915dd397e606"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set has reduced to:  111473\n",
            "Test set length:  10240\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting questions to array of integers\n",
        "for i in range(len(Q1_train)):\n",
        "    Q1_train[i] = [vocab[word] for word in Q1_train[i]]\n",
        "    Q2_train[i] = [vocab[word] for word in Q2_train[i]]\n",
        "\n",
        "\n",
        "for i in range(len(Q1_test)):\n",
        "    Q1_test[i] = [vocab[word] for word in Q1_test[i]]\n",
        "    Q2_test[i] = [vocab[word] for word in Q2_test[i]]"
      ],
      "metadata": {
        "id": "vgR5GPiSwA1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('first question in the train set:\\n')\n",
        "print(Q1_train_words[0], '\\n')\n",
        "print('encoded version:')\n",
        "print(Q1_train[0],'\\n')\n",
        "\n",
        "print('first question in the test set:\\n')\n",
        "print(Q1_test_words[0], '\\n')\n",
        "print('encoded version:')\n",
        "print(Q1_test[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1IzwPuiwMCa",
        "outputId": "ec50eb38-ce37-47e0-d3dc-3708de6d2692"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "first question in the train set:\n",
            "\n",
            "Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me? \n",
            "\n",
            "encoded version:\n",
            "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21] \n",
            "\n",
            "first question in the test set:\n",
            "\n",
            "What were some of the troubles you have faced during and after your 9 months period of pregnancy? \n",
            "\n",
            "encoded version:\n",
            "[30, 271, 116, 131, 78, 28882, 53, 218, 6595, 124, 11, 267, 56, 1636, 606, 2091, 131, 4329, 21]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cut_off = int(len(Q1_train)*.8)\n",
        "train_Q1, train_Q2 = Q1_train[:cut_off], Q2_train[:cut_off]\n",
        "val_Q1, val_Q2 = Q1_train[cut_off: ], Q2_train[cut_off:]\n",
        "print('Number of duplicate questions: ', len(Q1_train))\n",
        "print(\"The length of the training set is:  \", len(train_Q1))\n",
        "print(\"The length of the validation set is: \", len(val_Q1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2_HCnRQwSXS",
        "outputId": "f09733ca-f49b-497a-bafe-b8db71f2a6c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of duplicate questions:  111473\n",
            "The length of the training set is:   89178\n",
            "The length of the validation set is:  22295\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def data_generator(Q1, Q2, batch_size, pad=0, shuffle=True):\n",
        "    \"\"\"Generator function that yields batches of data\n",
        "\n",
        "    Args:\n",
        "        Q1 (list): List of transformed (to tensor) questions.\n",
        "        Q2 (list): List of transformed (to tensor) questions.\n",
        "        batch_size (int): Number of elements per batch.\n",
        "        pad (int, optional): Pad character from the vocab. Defaults to 1.\n",
        "        shuffle (bool, optional): If the batches should be randomnized or not. Defaults to True.\n",
        "    Yields:\n",
        "        tuple: Of the form (input1, input2) with types (numpy.ndarray, numpy.ndarray)\n",
        "        NOTE: input1: inputs to your model [q1a, q2a, q3a, ...] i.e. (q1a,q1b) are duplicates\n",
        "              input2: targets to your model [q1b, q2b,q3b, ...] i.e. (q1a,q2i) i!=a are not duplicates\n",
        "    \"\"\"\n",
        "\n",
        "    input1 = []\n",
        "    input2 = []\n",
        "    idx = 0\n",
        "    len_q = len(Q1)\n",
        "    question_indexes = [*range(len_q)]\n",
        "\n",
        "    if shuffle:\n",
        "        rnd.shuffle(question_indexes)\n",
        "\n",
        "    #Assuming true condition\n",
        "    while True:\n",
        "        if idx >= len_q:\n",
        "            # if idx is greater than or equal to len_q, set idx accordingly\n",
        "            # (Hint: look at the instructions above)\n",
        "            idx = 0\n",
        "            # shuffle to get random batches if shuffle is set to True\n",
        "            if shuffle:\n",
        "                rnd.shuffle(question_indexes)\n",
        "\n",
        "        # get questions at the `question_indexes[idx]` position in Q1 and Q2\n",
        "        q1 = Q1[question_indexes[idx]]\n",
        "        q2 = Q2[question_indexes[idx]]\n",
        "\n",
        "        # increment idx by 1\n",
        "        idx += 1\n",
        "        # append q1\n",
        "        input1.append(q1)\n",
        "        # append q2\n",
        "        input2.append(q2)\n",
        "        if len(input1) == batch_size:\n",
        "\n",
        "            max_len = max(max([len(q) for q in input1]),\n",
        "                          max([len(q) for q in input2]))\n",
        "            # pad to power-of-2 (Hint: look at the instructions above)\n",
        "            max_len = 2**int(np.ceil(np.log2(max_len)))\n",
        "            b1 = []\n",
        "            b2 = []\n",
        "            for q1, q2 in zip(input1, input2):\n",
        "                # add [pad] to q1 until it reaches max_len\n",
        "                q1 = q1 + [pad] * (max_len - len(q1))\n",
        "                q2 = q2 + [pad] * (max_len - len(q2))\n",
        "                # append q1\n",
        "                b1.append(q1)\n",
        "                b2.append(q2)\n",
        "            # use b1 and b2\n",
        "            yield np.array(b1), np.array(b2)\n",
        "\n",
        "            # reset the batches\n",
        "            input1, input2 = [], []"
      ],
      "metadata": {
        "id": "Hvbsc9CFwc3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 2\n",
        "res1, res2 = next(data_generator(train_Q1, train_Q2, batch_size))\n",
        "print(\"First questions  : \",'\\n', res1, '\\n')\n",
        "print(\"Second questions : \",'\\n', res2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwIrP8hG0D8W",
        "outputId": "d83f5763-8983-4097-992a-c4f1665285b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First questions  :  \n",
            " [[   30    87    78  1016    39  1608  1609    21     0     0     0     0\n",
            "      0     0     0     0]\n",
            " [   32   545    73   230 10094   229   231    21     0     0     0     0\n",
            "      0     0     0     0]] \n",
            "\n",
            "Second questions :  \n",
            " [[  30  156   78 3940 1918   39 1608 1609   21    0    0    0    0    0\n",
            "     0    0]\n",
            " [  32   38    4  229  230  231   28 2582    6  816   21    0    0    0\n",
            "     0    0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Siamese(vocab_size=len(vocab), d_model=128, mode='train'):\n",
        "\n",
        "    def normalize(x):  # normalizes the vectors to have L2 norm 1\n",
        "        return x / fastnp.sqrt(fastnp.sum(x * x, axis=-1, keepdims=True))\n",
        "\n",
        "   q_processor = tl.Serial(  # Processor will run on Q1 and Q2.\n",
        "        tl.Embedding(vocab_size, d_model),\n",
        "        # Run LSTM. If this is not dim d_model it raises an error\n",
        "        tl.LSTM(d_model),\n",
        "        # Average vectors on the length axis.\n",
        "        tl.Mean(axis=1),\n",
        "        tl.Fn('Normalize', lambda x: normalize(x))  # Apply normalize function\n",
        "    )\n",
        "\n",
        "    # Run on Q1 and Q2 in parallel.\n",
        "    model = tl.Parallel(q_processor, q_processor)\n",
        "    return model"
      ],
      "metadata": {
        "id": "QS7ZsuvU0Je7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Siamese()\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLHVkfZd0UkC",
        "outputId": "f7054b10-d31e-471c-a94d-329efddc679a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parallel_in2_out2[\n",
            "  Serial[\n",
            "    Embedding_41794_128\n",
            "    LSTM_128\n",
            "    Mean\n",
            "    Normalize\n",
            "  ]\n",
            "  Serial[\n",
            "    Embedding_41794_128\n",
            "    LSTM_128\n",
            "    Mean\n",
            "    Normalize\n",
            "  ]\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def TripletLossFn(v1, v2, margin=0.25):\n",
        "\n",
        "\n",
        "    # calculate new batch size\n",
        "    batch_size = len(scores)\n",
        "    # use fastnp to grab all postive `diagonal` entries in `scores`\n",
        "    positive = fastnp.diagonal(scores)  # the positive ones (duplicates)\n",
        "    # multiply `fastnp.eye(batch_size)` with 2.0 and subtract it out of `scores`\n",
        "    negative_without_positive = scores - 2.0 * fastnp.eye(batch_size)\n",
        "    # take the row by row `max` of `negative_without_positive`.\n",
        "    closest_negative = negative_without_positive.max(axis=1) # [batch]\n",
        "    # subtract `fastnp.eye(batch_size)` out of 1.0 and do element-wise multiplication with `scores`\n",
        "    negative_zero_on_duplicate = scores * (1.0 - fastnp.eye(batch_size))\n",
        "    # use `fastnp.sum` on `negative_zero_on_duplicate` for `axis=1` and divide it by `(batch_size - 1)`\n",
        "    mean_negative = np.sum(negative_zero_on_duplicate, axis=1) / (batch_size-1)\n",
        "    # compute `fastnp.maximum` among 0.0 and `A`\n",
        "    # A = subtract `positive` from `margin` and add `closest_negative`\n",
        "    triplet_loss1 = fastnp.maximum(0.0, margin - positive + closest_negative)\n",
        "    # compute `fastnp.maximum` among 0.0 and `B`\n",
        "    # B = subtract `positive` from `margin` and add `mean_negative`\n",
        "    triplet_loss2 = fastnp.maximum(0.0, margin - positive + mean_negative)\n",
        "    # add the two losses together and take the `fastnp.mean` of it\n",
        "    triplet_loss = fastnp.mean(triplet_loss1 + triplet_loss2)\n",
        "\n",
        "    return triplet_loss"
      ],
      "metadata": {
        "id": "4RqujzHp-zlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "v1 = np.array([[0.26726124, 0.53452248, 0.80178373],[0.5178918 , 0.57543534, 0.63297887]])\n",
        "v2 = np.array([[ 0.26726124,  0.53452248,  0.80178373],[-0.5178918 , -0.57543534, -0.63297887]])\n",
        "TripletLossFn(v2,v1)\n",
        "print(\"Triplet Loss:\", TripletLossFn(v2,v1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YCprcMOGhOD",
        "outputId": "2982825d-eebe-43f4-cb08-a65b21b13c39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Triplet Loss: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "#Partial function gives default values to parameters to check if the function is working correctly\n",
        "def TripletLoss(margin=0.25):\n",
        "    triplet_loss_fn = partial(TripletLossFn, margin=margin)\n",
        "    return tl.Fn('TripletLoss', triplet_loss_fn)"
      ],
      "metadata": {
        "id": "LFZ-khVvGo5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256\n",
        "train_generator = data_generator(train_Q1, train_Q2, batch_size, vocab['<PAD>'])\n",
        "val_generator = data_generator(val_Q1, val_Q2, batch_size, vocab['<PAD>'])\n",
        "print('train_Q1.shape ', train_Q1.shape)\n",
        "print('val_Q1.shape   ', val_Q1.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJwJrYBeJjyw",
        "outputId": "92e83db0-eae1-49ca-ae0c-279cd15eba1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_Q1.shape  (89178,)\n",
            "val_Q1.shape    (22295,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr_schedule = trax.lr.warmup_and_rsqrt_decay(400, 0.01)\n",
        "\n",
        "def train_model(Siamese, TripletLoss, lr_schedule, train_generator=train_generator, val_generator=val_generator, output_dir='model/'):\n",
        "\n",
        "    output_dir = os.path.expanduser(output_dir)\n",
        "\n",
        "    train_task = training.TrainTask(\n",
        "        labeled_data=train_generator,\n",
        "        loss_layer=TripletLoss(),\n",
        "        optimizer=trax.optimizers.Adam(0.01),\n",
        "        lr_schedule=lr_schedule,\n",
        "    )\n",
        "\n",
        "    eval_task = training.EvalTask(\n",
        "        labeled_data=val_generator,\n",
        "        metrics=[TripletLoss()],\n",
        "    )\n",
        "\n",
        "    training_loop = training.Loop(Siamese(),\n",
        "                                  train_task,\n",
        "                                  eval_tasks=eval_task,\n",
        "                                  output_dir=output_dir)\n",
        "\n",
        "    return training_loop"
      ],
      "metadata": {
        "id": "wPCAP4PE6RxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_steps = 1000\n",
        "training_loop = train_model(Siamese, TripletLoss, lr_schedule)\n",
        "training_loop.run(train_steps)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17KuMwRw8x5W",
        "outputId": "9a86e134-3729-42f0-93e2-0b74bc9ef0a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/jax/_src/lib/xla_bridge.py:435: UserWarning: jax.host_count has been renamed to jax.process_count. This alias will eventually be removed; please update your code.\n",
            "  \"jax.host_count has been renamed to jax.process_count. This alias \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step    400: Ran 100 train steps in 87.94 secs\n",
            "Step    400: train TripletLoss |  0.49996403\n",
            "Step    400: eval  TripletLoss |  0.49993986\n",
            "\n",
            "Step    500: Ran 100 train steps in 84.21 secs\n",
            "Step    500: train TripletLoss |  0.43893430\n",
            "Step    500: eval  TripletLoss |  0.38584650\n",
            "\n",
            "Step    600: Ran 100 train steps in 82.73 secs\n",
            "Step    600: train TripletLoss |  0.33849263\n",
            "Step    600: eval  TripletLoss |  0.32622865\n",
            "\n",
            "Step    700: Ran 100 train steps in 79.96 secs\n",
            "Step    700: train TripletLoss |  0.30007324\n",
            "Step    700: eval  TripletLoss |  0.27818257\n",
            "\n",
            "Step    800: Ran 100 train steps in 81.20 secs\n",
            "Step    800: train TripletLoss |  0.27914274\n",
            "Step    800: eval  TripletLoss |  0.27767241\n",
            "\n",
            "Step    900: Ran 100 train steps in 78.76 secs\n",
            "Step    900: train TripletLoss |  0.26637343\n",
            "Step    900: eval  TripletLoss |  0.25478059\n",
            "\n",
            "Step   1000: Ran 100 train steps in 83.86 secs\n",
            "Step   1000: train TripletLoss |  0.22693928\n",
            "Step   1000: eval  TripletLoss |  0.19278604\n",
            "\n",
            "Step   1100: Ran 100 train steps in 92.00 secs\n",
            "Step   1100: train TripletLoss |  0.15988921\n",
            "Step   1100: eval  TripletLoss |  0.13670610\n",
            "\n",
            "Step   1200: Ran 100 train steps in 83.44 secs\n",
            "Step   1200: train TripletLoss |  0.12766519\n",
            "Step   1200: eval  TripletLoss |  0.11698152\n",
            "\n",
            "Step   1300: Ran 100 train steps in 82.47 secs\n",
            "Step   1300: train TripletLoss |  0.11091768\n",
            "Step   1300: eval  TripletLoss |  0.10589582\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = Siamese()\n",
        "model1.init_from_file('/content/model.pkl.gz')"
      ],
      "metadata": {
        "id": "rdKYDEh_-Ois",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c69b5a6-3e4e-4075-8f42-926a0d69eda4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(((array([[-0.7434783 , -0.5323    ,  0.26556844, ...,  0.24734499,\n",
              "            0.971258  , -0.3176002 ],\n",
              "          [-1.9103315 , -1.2298064 ,  0.7929189 , ..., -1.3576206 ,\n",
              "           -0.9268899 ,  0.11710498],\n",
              "          [ 1.1356051 ,  1.2533569 ,  1.4670613 , ...,  1.2557949 ,\n",
              "            1.1703947 ,  1.7554839 ],\n",
              "          ...,\n",
              "          [-0.49271938,  0.06522572, -0.74080336, ...,  1.4723355 ,\n",
              "            1.1603701 ,  0.51038134],\n",
              "          [ 1.4607905 , -0.15703319,  0.5001072 , ...,  0.18419997,\n",
              "           -0.5392223 , -0.4307455 ],\n",
              "          [-1.4316021 , -1.2368174 ,  0.0611912 , ..., -0.24021208,\n",
              "            0.34730613, -0.07061554]], dtype=float32),\n",
              "   (((), ((), ())),\n",
              "    ((array([[-0.02550941, -0.06643244, -0.03194237, ..., -0.01651929,\n",
              "              -0.0235158 , -0.02485679],\n",
              "             [-0.0683428 , -0.06671927,  0.0349182 , ...,  0.01044205,\n",
              "               0.02273431,  0.0717328 ],\n",
              "             [-0.03171944,  0.01028203,  0.05781721, ..., -0.04236581,\n",
              "              -0.04657996, -0.06436575],\n",
              "             ...,\n",
              "             [ 0.03127091, -0.02159434, -0.08765983, ...,  0.02809162,\n",
              "              -0.01996687,  0.00975176],\n",
              "             [-0.02458269, -0.02480528, -0.07895178, ..., -0.08660002,\n",
              "               0.05699608, -0.02815895],\n",
              "             [ 0.03601498,  0.04997371,  0.02829743, ..., -0.0378585 ,\n",
              "              -0.08549955, -0.05847359]], dtype=float32),\n",
              "      array([0.9999873 , 0.99998873, 0.9999902 , 0.99998885, 0.9999903 ,\n",
              "             0.99999326, 0.999991  , 0.9999885 , 0.99998873, 0.9999922 ,\n",
              "             0.99999166, 0.9999899 , 0.99998844, 0.9999887 , 0.9999879 ,\n",
              "             0.9999915 , 0.9999861 , 0.9999893 , 0.9999898 , 0.99998873,\n",
              "             0.99998903, 0.9999907 , 0.9999904 , 0.9999898 , 0.9999872 ,\n",
              "             0.9999893 , 0.9999884 , 0.99999195, 0.9999887 , 0.99999106,\n",
              "             0.9999898 , 0.9999901 , 0.99998766, 0.9999907 , 0.9999862 ,\n",
              "             0.999988  , 0.9999887 , 0.9999905 , 0.9999898 , 0.99998724,\n",
              "             0.99998826, 0.99998754, 0.99998873, 0.9999895 , 0.9999895 ,\n",
              "             0.99999183, 0.9999906 , 0.999989  , 0.9999955 , 0.9999871 ,\n",
              "             0.99999   , 0.9999893 , 0.99999106, 0.9999881 , 0.99998856,\n",
              "             0.9999909 , 0.99998885, 0.9999881 , 0.99999106, 0.9999861 ,\n",
              "             0.999991  , 0.9999886 , 0.99999267, 0.9999936 , 0.9999905 ,\n",
              "             0.9999886 , 0.99999076, 0.99998826, 0.99998766, 0.9999887 ,\n",
              "             0.99998975, 0.9999886 , 0.9999902 , 0.9999874 , 0.99998903,\n",
              "             0.9999917 , 0.99998915, 0.99998915, 0.9999961 , 0.99998933,\n",
              "             0.9999876 , 0.99998784, 0.9999894 , 0.999991  , 0.9999883 ,\n",
              "             0.99999297, 0.99999213, 0.9999894 , 0.99998635, 0.9999892 ,\n",
              "             0.9999884 , 0.9999898 , 0.99999076, 0.99999064, 0.99998987,\n",
              "             0.999991  , 0.9999883 , 0.9999901 , 0.9999894 , 0.9999873 ,\n",
              "             0.9999905 , 0.999988  , 0.9999894 , 0.99998724, 0.9999887 ,\n",
              "             0.99998856, 0.9999896 , 0.99999005, 0.9999914 , 0.99999017,\n",
              "             0.9999878 , 0.9999874 , 0.9999894 , 0.9999904 , 0.9999885 ,\n",
              "             0.99999094, 0.9999902 , 0.9999837 , 0.9999906 , 0.99999326,\n",
              "             0.99999076, 0.9999874 , 0.9999915 , 0.99998766, 0.99999183,\n",
              "             0.9999902 , 0.9999897 , 0.999988  , 0.99997586, 0.99998116,\n",
              "             0.99998844, 0.99997693, 0.99998796, 0.9999755 , 0.9999877 ,\n",
              "             0.9999863 , 0.9999866 , 1.0000036 , 0.9999898 , 0.9999837 ,\n",
              "             0.9999796 , 0.99997854, 0.99998003, 0.99999166, 0.99998474,\n",
              "             0.99998194, 0.99998844, 0.9999833 , 0.9999866 , 0.99998385,\n",
              "             0.9999883 , 0.9999821 , 0.99998605, 0.9999878 , 0.9999786 ,\n",
              "             0.99997604, 0.99998504, 0.9999835 , 0.99998707, 0.9999886 ,\n",
              "             0.99998134, 0.9999869 , 0.9999835 , 0.9999783 , 0.99998134,\n",
              "             0.9999805 , 0.9999862 , 0.99998933, 0.99998444, 0.999987  ,\n",
              "             0.99998605, 0.99998814, 0.99999154, 0.99998987, 0.9999869 ,\n",
              "             0.9999892 , 0.9999964 , 0.999978  , 0.99998164, 0.9999928 ,\n",
              "             0.99998844, 0.9999784 , 0.99998164, 0.99998116, 0.99998176,\n",
              "             0.99998647, 0.9999849 , 0.99998003, 0.9999817 , 0.9999905 ,\n",
              "             0.99998236, 0.9999761 , 0.99998266, 0.99999106, 0.9999891 ,\n",
              "             0.9999774 , 0.99998754, 0.99998385, 0.9999859 , 0.9999839 ,\n",
              "             0.9999906 , 0.9999829 , 0.9999904 , 0.9999824 , 0.9999845 ,\n",
              "             0.9999874 , 0.99997836, 0.9999874 , 0.9999776 , 0.999985  ,\n",
              "             0.99999505, 0.9999806 , 0.9999777 , 0.99997616, 0.9999886 ,\n",
              "             0.99998724, 0.9999794 , 0.99998355, 0.9999879 , 0.9999827 ,\n",
              "             0.99999166, 0.9999827 , 0.9999897 , 0.99998856, 0.99998623,\n",
              "             0.9999871 , 0.99998367, 0.9999769 , 0.99998647, 0.9999796 ,\n",
              "             0.99997723, 0.99998975, 0.9999881 , 0.9999898 , 0.9999866 ,\n",
              "             0.99999917, 0.99998224, 0.99998546, 0.9999798 , 0.9999887 ,\n",
              "             0.99999523, 0.9999873 , 0.99998844, 0.99998856, 0.99998164,\n",
              "             0.9999799 , 0.9999893 , 0.99999726, 0.9999856 , 0.9999798 ,\n",
              "             0.9999885 , 0.9999828 , 0.99997884, 0.99997747, 0.99998266,\n",
              "             0.9999813 , 0.9999861 , 0.9999872 , 0.99998856, 0.9999861 ,\n",
              "             0.9999863 , 0.99999833, 0.9999936 , 0.999989  , 0.99999154,\n",
              "             0.9999952 , 0.99999017, 0.99998707, 0.99998784, 0.9999849 ,\n",
              "             0.99998766, 0.99999076, 0.99998665, 0.99999154, 0.99998957,\n",
              "             0.9999874 , 0.9999893 , 0.9999904 , 0.99999046, 0.9999862 ,\n",
              "             0.9999882 , 0.99998885, 0.9999869 , 0.99999475, 0.9999882 ,\n",
              "             0.9999891 , 0.99998987, 0.99998945, 0.9999873 , 0.9999898 ,\n",
              "             0.9999863 , 0.99998856, 0.9999844 , 0.99999213, 0.99999046,\n",
              "             0.9999886 , 0.99998766, 0.9999888 , 0.99998873, 0.9999884 ,\n",
              "             0.999989  , 0.9999942 , 0.99998945, 0.9999929 , 0.9999983 ,\n",
              "             0.9999856 , 0.99998534, 0.99999005, 0.9999897 , 0.9999868 ,\n",
              "             0.9999875 , 0.99998844, 0.99999   , 0.9999888 , 0.99998873,\n",
              "             0.99998474, 0.9999915 , 0.99998856, 0.99999547, 0.9999969 ,\n",
              "             0.99998856, 0.9999887 , 0.99999046, 0.9999899 , 0.99998975,\n",
              "             0.9999856 , 0.99999005, 0.99998456, 0.9999874 , 0.99998575,\n",
              "             0.99999005, 0.99999505, 0.99998885, 0.9999906 , 1.0000011 ,\n",
              "             0.99998915, 0.999983  , 0.99998856, 0.9999981 , 0.9999888 ,\n",
              "             0.9999902 , 0.99999684, 0.99998987, 0.99998665, 0.99998504,\n",
              "             0.9999884 , 0.9999873 , 0.9999863 , 0.9999907 , 0.9999888 ,\n",
              "             0.9999894 , 0.9999893 , 0.9999908 , 0.99999124, 0.9999887 ,\n",
              "             0.9999851 , 0.9999908 , 0.9999859 , 0.9999894 , 0.9999879 ,\n",
              "             0.99998957, 0.9999904 , 0.9999875 , 1.0000012 , 0.999993  ,\n",
              "             0.9999875 , 0.9999866 , 0.9999902 , 0.9999909 , 0.9999873 ,\n",
              "             0.9999889 , 0.99999005, 0.9999866 , 0.99998224, 0.9999889 ,\n",
              "             0.99999666, 0.9999897 , 0.99998766, 0.99999106, 0.9999863 ,\n",
              "             0.9999922 , 0.9999883 , 0.99998796, 0.999989  , 0.99999154,\n",
              "             0.9999906 , 0.9999861 , 0.9999887 , 0.9999845 , 0.9999956 ,\n",
              "             0.99999326, 0.9999852 , 0.99999094, 0.9999919 , 0.9999905 ,\n",
              "             0.9999917 , 0.9999908 , 0.99998504, 0.9999863 , 0.99999243,\n",
              "             0.9999867 , 0.99999386, 0.99998814, 0.99998605, 0.9999883 ,\n",
              "             0.99999666, 0.9999906 , 0.99998415, 0.9999882 , 0.999987  ,\n",
              "             0.9999912 , 0.99999535, 0.99999213, 0.9999912 , 0.9999879 ,\n",
              "             0.9999851 , 0.9999897 , 0.9999888 , 0.99998695, 0.9999934 ,\n",
              "             0.99998665, 0.99999374, 0.99999285, 0.9999873 , 0.99998945,\n",
              "             0.99998707, 0.99998873, 0.9999887 , 0.99998784, 0.99999064,\n",
              "             0.99998647, 0.99999374, 0.9999962 , 0.99998885, 0.9999937 ,\n",
              "             0.99999094, 0.99998695, 0.99998724, 0.99998766, 0.99999255,\n",
              "             0.9999899 , 0.99998575, 0.999988  , 0.9999914 , 0.9999941 ,\n",
              "             0.9999882 , 0.99999523, 0.99999946, 0.9999908 , 0.9999871 ,\n",
              "             0.9999877 , 0.9999923 , 0.99998945, 0.9999867 , 0.9999863 ,\n",
              "             0.99998707, 0.99998933, 0.999985  , 0.999988  , 0.99999607,\n",
              "             0.9999861 , 0.9999903 , 1.0000001 , 0.9999924 , 0.9999875 ,\n",
              "             0.999987  , 0.9999967 , 0.9999887 , 0.9999927 , 0.9999978 ,\n",
              "             0.99999154, 0.99998736, 0.9999856 , 0.9999929 , 0.99998945,\n",
              "             0.9999885 , 0.99998915, 0.9999912 , 0.99998707, 0.9999867 ,\n",
              "             0.99999267, 0.9999922 , 0.9999915 , 0.999992  , 0.9999887 ,\n",
              "             0.9999879 , 0.9999926 , 0.9999883 , 0.9999881 , 0.9999919 ,\n",
              "             0.9999901 , 1.0000008 , 0.9999919 , 0.9999881 , 0.9999875 ,\n",
              "             0.9999878 , 0.99999136, 0.9999868 , 0.99998736, 0.99999213,\n",
              "             0.9999868 , 0.9999882 , 0.9999884 , 0.9999947 , 0.9999933 ,\n",
              "             0.9999927 , 0.9999933 , 0.9999894 , 0.9999923 , 0.9999892 ,\n",
              "             0.99998623, 0.9999848 ], dtype=float32)),),\n",
              "    ()),\n",
              "   (),\n",
              "   ()),\n",
              "  {'__marker_for_cached_weights_': ()}),\n",
              " (((), (((), ((), ())), ((), ()), ()), (), ()),\n",
              "  {'__marker_for_cached_state_': ()}))"
            ]
          },
          "metadata": {},
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(question1, question2, threshold, model, vocab, data_generator=data_generator, verbose=False):\n",
        "\n",
        "    q1 = nltk.word_tokenize(question1)  # tokenize\n",
        "    q2 = nltk.word_tokenize(question2)  # tokenize\n",
        "    Q1, Q2 = [], []\n",
        "    for word in q1:  # encode q1\n",
        "        Q1 += [vocab[word]]\n",
        "    for word in q2:  # encode q2\n",
        "        Q2 += [vocab[word]]\n",
        "\n",
        "    Q1, Q2 = next(data_generator([Q1], [Q2], 1, vocab['<PAD>']))\n",
        "    # Call the model\n",
        "    v1, v2 = model1((Q1, Q2))\n",
        "    d = np.dot(v1[0], v2[0].T)\n",
        "        res = d > threshold\n",
        "\n",
        "    if(verbose):\n",
        "        print(\"Q1  = \", Q1, \"\\nQ2  = \", Q2)\n",
        "        print(\"d   = \", d)\n",
        "        print(\"res = \", res)\n",
        "\n",
        "    return res"
      ],
      "metadata": {
        "id": "K3P28fWAQsQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ov2uHI_eUWf3",
        "outputId": "ae996c76-504c-4a7b-ae8d-49ace67f46b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 4.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 5.9 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 37.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 48.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 48.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.12.1 transformers-4.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "% pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eja7cOwshqvY",
        "outputId": "58e3e609-addf-4ee5-8b5a-069d4b54a3d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 3.9 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers torch sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJgUYCgHfhoc",
        "outputId": "23703cda-40f7-4c35-f559-2a777e6cb235"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.18.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.5.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# using pipeline API for summarization task\n",
        "summarization = pipeline(\"summarization\") #pipeline is a model,summarization is a method in it\n",
        "original_text = \"\"\"\n",
        "Paul Walker is hardly the first actor to die during a production.\n",
        "But Walker's death in November 2013 at the age of 40 after a car crash was especially eerie given his rise to fame in the \"Fast and Furious\" film franchise.\n",
        "The release of \"Furious 7\" on Friday offers the opportunity for fans to remember -- and possibly grieve again -- the man that so many have praised as one of the nicest guys in Hollywood.\n",
        "\"He was a person of humility, integrity, and compassion,\" military veteran Kyle Upham said in an email to CNN.\n",
        "Walker secretly paid for the engagement ring Upham shopped for with his bride.\n",
        "\"We didn't know him personally but this was apparent in the short time we spent with him.\n",
        "I know that we will never forget him and he will always be someone very special to us,\" said Upham.\n",
        "The actor was on break from filming \"Furious 7\" at the time of the fiery accident, which also claimed the life of the car's driver, Roger Rodas.\n",
        "Producers said early on that they would not kill off Walker's character, Brian O'Connor, a former cop turned road racer. Instead, the script was rewritten and special effects were used to finish scenes, with Walker's brothers, Cody and Caleb, serving as body doubles.\n",
        "There are scenes that will resonate with the audience -- including the ending, in which the filmmakers figured out a touching way to pay tribute to Walker while \"retiring\" his character. At the premiere Wednesday night in Hollywood, Walker's co-star and close friend Vin Diesel gave a tearful speech before the screening, saying \"This movie is more than a movie.\" \"You'll feel it when you see it,\" Diesel said. \"There's something emotional that happens to you, where you walk out of this movie and you appreciate everyone you love because you just never know when the last day is you're gonna see them.\" There have been multiple tributes to Walker leading up to the release. Diesel revealed in an interview with the \"Today\" show that he had named his newborn daughter after Walker.\n",
        "Social media has also been paying homage to the late actor. A week after Walker's death, about 5,000 people attended an outdoor memorial to him in Los Angeles. Most had never met him. Marcus Coleman told CNN he spent almost $1,000 to truck in a banner from Bakersfield for people to sign at the memorial. \"It's like losing a friend or a really close family member ... even though he is an actor and we never really met face to face,\" Coleman said. \"Sitting there, bringing his movies into your house or watching on TV, it's like getting to know somebody. It really, really hurts.\" Walker's younger brother Cody told People magazine that he was initially nervous about how \"Furious 7\" would turn out, but he is happy with the film. \"It's bittersweet, but I think Paul would be proud,\" he said. CNN's Paul Vercammen contributed to this report.\n",
        "\"\"\"\n",
        "summary_text1 = summarization(original_text)[0]['summary_text'] # giving input\n",
        "print(\"Summary:\", summary_text1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fquwM28hfqUZ",
        "outputId": "9a41f926-590c-4e5e-fd5d-9a416dca6ae2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 (https://huggingface.co/sshleifer/distilbart-cnn-12-6)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary:  Paul Walker died in November 2013 after a car crash in Los Angeles . The late actor was one of the nicest guys in Hollywood . The release of \"Furious 7\" on Friday offers a chance to grieve again . There have been multiple tributes to Walker leading up to the film's release .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary_text1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "QHjaHYUCiqW8",
        "outputId": "7655a7a3-dce1-4b55-ab94-51b8a7070edf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Paul Walker died in November 2013 after a car crash in Los Angeles . The late actor was one of the nicest guys in Hollywood . The release of \"Furious 7\" on Friday offers a chance to grieve again . There have been multiple tributes to Walker leading up to the film\\'s release .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "original_text = \"\"\"Paul Walker met an untimely death.\n",
        "\n",
        "Known for being a generous philanthropist, Walker had spent November 30, 2013, at a toy drive event for his disaster relief charity, Reach Out Worldwide, which was founded in the wake of the 2010 earthquake in Haiti. Walker left happily just before 3:30 p.m. — and he was never seen alive again.The pair left the event in a 2005 Porsche Carrera GT, with Rodas driving and Walker riding shotgun. The car was known for being hard to handle, and only a few hundred yards away from the shop, Rodas lost control of the vehicle. The Porsche was traveling at about 100 miles per hour before it hit a curb, a tree, a light post, and then another tree before bursting into flames.\"\"\""
      ],
      "metadata": {
        "id": "TIByZsL5jCNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_text2 = summarization(original_text)[0]['summary_text']\n",
        "print(\"Summary:\", summary_text2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZSGOzf9krAu",
        "outputId": "88b3be3a-5c21-4287-87f1-db1be1493966"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary:  Paul Walker died in November 2013 after a car crash in Los Angeles . The late actor was one of the nicest guys in Hollywood . The release of \"Furious 7\" on Friday offers a chance to grieve again . There have been multiple tributes to Walker leading up to the film's release .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Python3 code to demonstrate\n",
        "# removal of bad_chars\n",
        "# using replace()\n",
        "\n",
        "# initializing bad_chars_list\n",
        "bad_chars = [';', ':', '!','-','\"',\"*\"]\n",
        "\n",
        "# initializing test string\n",
        "test_string1 = summary_text1\n",
        "test_string2 = summary_text2\n",
        "# printing original string\n",
        "print (\"Original String : \" + test_string1)\n",
        "print (\"Original String : \" + test_string2)\n",
        "# using replace() to\n",
        "# remove bad_chars\n",
        "for i in bad_chars :\n",
        "    test_string = test_string.replace(i, '')\n",
        "\n",
        "# printing resultant string\n",
        "print (\"Resultant string1 is : \" + test_string1)\n",
        "print (\"Resultant string2 is : \" + test_string2 )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tp94u9VulYhn",
        "outputId": "19865e5f-0389-4e19-cba0-ca7e37e87e76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original String :  Paul Walker died in November 2013 after a car crash in Los Angeles . The late actor was one of the nicest guys in Hollywood . The release of \"Furious 7\" on Friday offers a chance to grieve again . There have been multiple tributes to Walker leading up to the film's release .\n",
            "Original String :  Paul Walker died in November 2013 after a car crash in Los Angeles . The late actor was one of the nicest guys in Hollywood . The release of \"Furious 7\" on Friday offers a chance to grieve again . There have been multiple tributes to Walker leading up to the film's release .\n",
            "Resultant string1 is :  Paul Walker died in November 2013 after a car crash in Los Angeles . The late actor was one of the nicest guys in Hollywood . The release of \"Furious 7\" on Friday offers a chance to grieve again . There have been multiple tributes to Walker leading up to the film's release .\n",
            "Resultant string2 is :  Paul Walker died in November 2013 after a car crash in Los Angeles . The late actor was one of the nicest guys in Hollywood . The release of \"Furious 7\" on Friday offers a chance to grieve again . There have been multiple tributes to Walker leading up to the film's release .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question1 = test_string1\n",
        "question2 = test_string2\n",
        "\n",
        "predict(question1 , question2, 0.7, model1, vocab, verbose = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zimIp4FfoN4y",
        "outputId": "e189d1f1-b941-4d81-a49f-b2ee95f4114d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q1  =  [[ 9030 13394  1870    28  1872  5221   267     6  1708  6584    28  9521\n",
            "   9522   148  2366   960  4068    55   111   131    78  1857  2059    28\n",
            "    750   148  2366  3398   131   619     0  1300   622    72  4159  3622\n",
            "      6  1019    39     0  7287   148  5799   218  1496  2816     0    39\n",
            "  13394  6870   495    39    78  2061   254  3398   148     1     1     1\n",
            "      1     1     1     1]] \n",
            "Q2  =  [[ 9030 13394  1870    28  1872  5221   267     6  1708  6584    28  9521\n",
            "   9522   148  2366   960  4068    55   111   131    78  1857  2059    28\n",
            "    750   148  2366  3398   131   619     0  1300   622    72  4159  3622\n",
            "      6  1019    39     0  7287   148  5799   218  1496  2816     0    39\n",
            "  13394  6870   495    39    78  2061   254  3398   148     1     1     1\n",
            "      1     1     1     1]]\n",
            "d   =  1.0\n",
            "res =  True\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#AS BOTH THE PARAGRAPHS ARE OF SAME CONTEXT THE SIMILARITY IS RESULTED IN 1(MAX)."
      ],
      "metadata": {
        "id": "Xi-aTtgBq_PL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}